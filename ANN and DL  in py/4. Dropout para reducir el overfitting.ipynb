{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"#113D68\" size=6>Deep Learning con Python y Keras</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Parte 4. MLP avanzado</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=4>4. Dropout para reducir el overfitting</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. Introducción](#section1)\n",
    "* [2. Regularización por dropout](#section2)\n",
    "* [3. Dropout en la capa visible](#section3)\n",
    "* [4. Dropout en capas ocultas](#section4)\n",
    "* [5. Consejos al utilizar Dropout](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección, descubrirá la técnica de regularización de _Dropout_ y cómo aplicarla. Después de completar esta lección, sabrá:\n",
    "* Cómo funciona la técnica de regularización de _Dropout._\n",
    "* Cómo utilizar _Dropout_ en sus capas de entrada.\n",
    "* Cómo utilizar _Dropout_ en sus capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Eliminar warning\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. Introducción</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout es una técnica en la que se ignoran las neuronas seleccionadas al azar durante el entrenamiento. Se eliminan al azar. Esto significa que su contribución a la activación de las neuronas siguientes se elimina temporalmente en el pase hacia adelante y las actualizaciones de peso no se aplican a la neurona en el pase hacia atrás.\n",
    "\n",
    "Esta dependencia del contexto de una neurona durante el entrenamiento se denomina _coadaptaciones complejas._ Entonces si las neuronas se eliminan aleatoriamente, otras neuronas tendrán que intervenir y manejar la representación requerida para hacer predicciones para las neuronas faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre el artículo [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. Regularización por Dropout</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Dropout_ se implementa fácilmente seleccionando aleatoriamente los nodos que se eliminarán con una probabilidad determinada (por ejemplo, 20%) en cada ciclo de actualización de peso.\n",
    "\n",
    "Los ejemplos utilizarán el conjunto de datos de clasificación binaria del conjunto de datos de Sonar. \n",
    "\n",
    "El modelo de red neuronal de línea de base tiene:\n",
    "1. Dos capas ocultas, la primera con 60 neuronas y la segunda con 30. \n",
    "2. Gradiente Descendiente Estocástico se utiliza para el entrenamiento con una tasa de aprendizaje y _momentum_ relativamente bajos. \n",
    "\n",
    "Veamos los resultados de linea base sin _Dropout._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de linea base: 85.55% (8.31%)\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model on the Sonar Dataset\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "r = \"C:/Users/Maria Luisa/OneDrive/Documentos/Cursos/DeepLearningConPythonyKerasRedesNeuronalesAvanzado/DropOut_para_generalizarmodelo/Datasets/sonar.csv\"\n",
    "df = pd.read_csv(r, header=None)\n",
    "df_values = df.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df_values[:, 0:60].astype(float)\n",
    "y = df_values[:, 60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y =encoder.transform(y)\n",
    "\n",
    "# baseline\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim = 60, activation = 'relu'))\n",
    "    model.add(Dense(30, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    sgd = SGD(lr = 0.01, momentum = 0.8)\n",
    "    model.compile(loss='binary_crossentropy', optimizer = sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "# Results\n",
    "estimator = []\n",
    "estimator.append(('standarize', StandardScaler()))\n",
    "estimator.append(('MLP', KerasClassifier(build_fn = baseline_model,\n",
    "                                        epochs=300, \n",
    "                                        batch_size=16,\n",
    "                                        verbose=0)))\n",
    "pipeline= Pipeline(estimator)\n",
    "kfold = StratifiedKFold(n_splits= 10, shuffle=True)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_y, cv=kfold)\n",
    "print('Modelo de linea base: %.2f%% (%.2f%%)'%(results.mean()*100, \n",
    "                                              results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. Dropout en la capa visible</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Dropout_ se puede aplicar a las neuronas de entrada. En el siguiente ejemplo: \n",
    "\n",
    "1. Agregamos una nueva capa de exclusión entre la entrada (o capa visible) y la primera capa oculta. \n",
    "2. La tasa de _Dropout_ se establece en 20%, i.e., una de cada cinco entradas se excluirá al azar.\n",
    "3. Se impone una restricción en los pesos para cada capa oculta con la norma máxima de los pesos para que no exceda de 3. \n",
    "    * Esto se hace estableciendo el argumento de `kernel_constraint` en el clase `Dense` al construir las capas. \n",
    "4. Aumentamos la tasa de aprendizaje y momentum. \n",
    "\n",
    "Vemoas el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo con Dropout en la entrada: 88.86% (10.38%)\n"
     ]
    }
   ],
   "source": [
    "# Example of Dropout on the Sonar Dataset: Visible Layer\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "r = \"C:/Users/Maria Luisa/OneDrive/Documentos/Cursos/DeepLearningConPythonyKerasRedesNeuronalesAvanzado/DropOut_para_generalizarmodelo/Datasets/sonar.csv\"\n",
    "df = pd.read_csv(r, header=None)\n",
    "df_values = df.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df_values[:, 0:60].astype(float)\n",
    "y = df_values[:, 60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y =encoder.transform(y)\n",
    "\n",
    "# baseline\n",
    "def input_dropout_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(60,)))\n",
    "    model.add(Dense(60, activation = 'relu',\n",
    "                    kernel_constraint = maxnorm(3)))\n",
    "    model.add(Dense(30, activation = 'relu',\n",
    "                   kernel_constraint = maxnorm(3)))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    sgd = SGD(lr = 0.01, momentum = 0.9)\n",
    "    model.compile(loss='binary_crossentropy', optimizer = sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "# Results\n",
    "estimator = []\n",
    "estimator.append(('standarize', StandardScaler()))\n",
    "estimator.append(('MLP', KerasClassifier(build_fn = input_dropout_model,\n",
    "                                        epochs=300, \n",
    "                                        batch_size=16,\n",
    "                                        verbose=0)))\n",
    "pipeline= Pipeline(estimator)\n",
    "kfold = StratifiedKFold(n_splits= 10, shuffle=True)\n",
    "\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_y, cv=kfold)\n",
    "print('Modelo con Dropout en la entrada: %.2f%% (%.2f%%)'%(results.mean()*100, \n",
    "                                              results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\" size=6>4. Dropout en capas ocultas</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Dropout_ se aplica también a las capas ocultas. En el siguiente ejemplo:\n",
    "\n",
    "1. _Dropout_ se aplica entre las dos capas ocultas y entre la última capa oculta y la capa de salida. \n",
    "2. Nuevamente, se usa una tasa de _Dropout_ del 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo con Dropout en la entrada: 85.12% (8.34%)\n"
     ]
    }
   ],
   "source": [
    "# Example of Dropout on the Sonar Dataset: Visible Layer\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "r = \"C:/Users/Maria Luisa/OneDrive/Documentos/Cursos/DeepLearningConPythonyKerasRedesNeuronalesAvanzado/DropOut_para_generalizarmodelo/Datasets/sonar.csv\"\n",
    "df = pd.read_csv(r, header=None)\n",
    "df_values = df.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df_values[:, 0:60].astype(float)\n",
    "y = df_values[:, 60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y =encoder.transform(y)\n",
    "\n",
    "# baseline\n",
    "def hidden_dropout_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, activation = 'relu',\n",
    "                    kernel_constraint = maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, activation = 'relu',\n",
    "                   kernel_constraint = maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    sgd = SGD(lr = 0.01, momentum = 0.9)\n",
    "    model.compile(loss='binary_crossentropy', optimizer = sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "# Results\n",
    "estimator = []\n",
    "estimator.append(('standarize', StandardScaler()))\n",
    "estimator.append(('MLP', KerasClassifier(build_fn = hidden_dropout_model,\n",
    "                                        epochs=300, \n",
    "                                        batch_size=16,\n",
    "                                        verbose=0)))\n",
    "pipeline= Pipeline(estimator)\n",
    "kfold = StratifiedKFold(n_splits= 10, shuffle=True)\n",
    "\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_y, cv=kfold)\n",
    "print('Modelo con Dropout en las capas ocultas: %.2f%% (%.2f%%)'%(results.mean()*100, \n",
    "                                              results.std()*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "# <font color=\"#004D7F\" size=6>5. Consejos al utilizar Dropout</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El documento original sobre _Dropout_ proporciona resultados experimentales sobre un conjunto de problemas estándar de Machine Learning. Como resultado, proporcionan una serie de heurísticas útiles a considerar cuando se usa la deserción en la práctica:\n",
    "* Por lo general, utilice un pequeño valor de _Dropout_ del 20% al 20%-50% como punto de partida.\n",
    "* Utilizar una red más grande.\n",
    "* Utilizar _Dropout_ en la entrada (capa visible) y capas ocultas. \n",
    "* Utilizar una gran tasa de aprendizaje (aumentar el factor de 10 a 100) y un gran momentum (0.9 a 0.99). \n",
    "* Restrinja el tamaño de los pesos de la red. Una gran tasa de aprendizaje puede resultar en pesos de red muy grandes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
