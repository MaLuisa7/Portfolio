{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"#113D68\" size=6>Deep Learning con Python</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Parte 5. Redes Neuronales Convolucionales</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=4>5. Análisis opiniones de películas</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. Conjunto de datos](#section1)\n",
    "    * [1.1. Cargar el dataset IMDB](#section1.1)\n",
    "* [2. Incrustar palabras](#section2)\n",
    "* [3. Multilayer Perceptron](#section3)\n",
    "    * [3.1. Librería y dataset](#section3.1)\n",
    "    * [3.2. Extraer número de reseñas](#section3.2)\n",
    "    * [3.3. Desarrollar MLP](#section3.3)\n",
    "    * [3.4. Evaluación del modelo](#section3.4)\n",
    "    * [2.5. Evaluación del modelo](#section3.5)\n",
    "* [4. CNN Unidimensional](#section4)\n",
    "    * [4.1. Librería y dataset](#section4.1)\n",
    "    * [4.2. Desarrollar la CNN](#section4.2)\n",
    "    * [4.3. Evaluación del modelo](#section4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección, descubriremos cómo puede predecir las opiniones de las críticas de películas como positivo o negativo:\n",
    "* Dataset IMDB para el procesamiento del lenguaje natural.\n",
    "* Usar la inserción de palabras para problemas de lenguaje natural.\n",
    "* Desarrollar y evaluar un modelo de perceptrón multicapa clásico para NLP.\n",
    "* Desarrollar una CNN unidimensional para NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Eliminar warning\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. Conjunto de datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos de IMDB contiene 50.000 reseñas de películas (buenas o malas) para entrenamiento y la misma cantidad para test. El problema es determinar si una crítica de película determinada tiene un sentimiento positivo o negativo.\n",
    "\n",
    "Originalmente se logró un Accuracy del 88,89%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puede obtener más información sobre el dataset [IMBD](http://ai.stanford.edu/~amaas/data/sentiment/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puede obtener más información sobre el artículo original [_Learning Word Vectors for Sentiment Analysis_](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.1. Cargar el dataset IMDB</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `imdb.load data()` permite cargar y descargar (la primera vez) IMDB. Lo almacenará en su directorio personal en `~/.keras/datasets/imdb.pkl` como un archivo de 32 megabytes. \n",
    "\n",
    "Las oraciones de cada revisión se componen de una secuencia de números enteros.\n",
    "\n",
    "De manera útil, la función `imdb.load data()` proporciona argumentos adicionales, como:\n",
    "1. El número de palabras principales para cargar.\n",
    "2. El número de palabras principales para omitir (como _\"the's\")_ \n",
    "3. La duración máxima de las revisiones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puede obtener más información sobre el dataset IMDB de [Keras](https://keras.io/api/datasets/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\Maria Luisa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Maria Luisa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# Load and Plot the IMDB dataset\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos el tamaño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " ...\n",
      " list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518])\n",
      " list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470])\n",
      " list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])]\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(np.unique(y))\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos los valores de clase únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos el número total de palabras únicas en el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88585\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(len(np.unique(np.hstack(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vemos la longitud media de las reseñas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de palabras : 234.76 (172.911495) \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "result=[len(x) for x in X]\n",
    "print('Media de palabras : %.2f (%f) ' %(np.mean(result), np.std(result)))\n",
    "\n",
    "# plot review length as a boxplot and histogram\n",
    "plt.subplot(121)\n",
    "plt.boxplot(result)\n",
    "plt.subplot(122)\n",
    "plt.hist(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. Incrustar palabras</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un avance reciente en el campo del NLP es la incrustación de palabras _(word embeddings)._ \n",
    "* Se trata de una técnica en la que las palabras se codifican como vectores de valor real en un espacio de alta dimensión, donde la similitud entre palabras en términos de significado se traduce en cercanía en el espacio vectorial. \n",
    "* Las palabras discretas se asignan a vectores de números continuos. \n",
    "\n",
    "Keras proporciona una forma conveniente de convertir representaciones enteras positivas de palabras en una incrustación de palabras mediante una capa `Embedding` que toma argumentos que:\n",
    "* Definen el mapeo, incluido el número máximo de palabras esperadas, también llamado tamaño de vocabulario. \n",
    "* También le permite especificar la dimensionalidad de cada vector de palabra, denominada dimensión de salida.\n",
    "\n",
    "Supongamos que solo nos interesan las primeras 5,000 palabras más utilizadas en el conjunto de datos. Por lo tanto:\n",
    "* Nuestro tamaño de vocabulario será de 5,000. \n",
    "* Podemos optar por utilizar un vector de 32 dimensiones para representar cada palabra. \n",
    "* Finalmente, podemos optar por limitar la extensión máxima de la reseña a 500 palabras, truncando las reseñas más largas y rellenando las reseñas más cortas con valores 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar el dataset:\n",
    "\n",
    "```python\n",
    "    imdb.load_data(num_words=5000)\n",
    "```\n",
    "\n",
    "Luego, Truncar o rellenar  una longitud de 500 usando la función `sequence.pad_sequences()`.\n",
    "\n",
    "```python\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "```\n",
    "\n",
    "Finalmente, la primera capa de nuestro modelo sería una capa de incrustación de palabras usando la clase `Embedding`:\n",
    "\n",
    "```python\n",
    "    Embedding(5000, 32, input_length=500)\n",
    "```\n",
    "\n",
    "La salida de esta primera capa sería una matriz con el tamaño de $32 × 500$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puede obtener más información sobre la clase [Embedding](http://keras.io/layers/embeddings/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. Multilayer Perceptron</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comenzar desarrollando un modelo MLP con una sola capa oculta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.1. Librería y dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el conjunto de datos IMDB y lo simplificaremos con las 5.000 palabras principales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\Maria Luisa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Maria Luisa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "vocab = 500\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words= vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.2\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.2. Extraer número de reseñas</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitamos las reseñas a 500 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.3\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.3. Desarrollar MLP</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear nuestro modelo. Usaremos\n",
    "1. Capa Embedding como entrada, estableciendo el vocabulario en 5,000, el tamaño del vector de la palabra en 32 dimensiones y la longitud de entrada en 500. \n",
    "2. La salida de esta primera capa será una matriz de tamaño $32 × 500$.\n",
    "3. Capa Flatten para la salida de las capas Embedding a una dimensión.\n",
    "4. Capa oculta densa de 250 neuronas con una función de activación ReLU. \n",
    "5. La capa de salida tiene 1 neurona con una activación sigmoidea para generar valores de 0 y 1. \n",
    "6. Funcioń de pérdida logarítmica y se optimiza mediante ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           16000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,016,501\n",
      "Trainable params: 4,016,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model =Sequential()\n",
    "model.add(Embedding(vocab, 32, input_length = max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation= 'relu'))\n",
    "model.add(Dense(1, activation ='sigmoid'))\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.4\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.4. Evaluación del modelo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos muy pocas épocas de entrenamiento, en este caso solo 2 y un tamaño de batch de 128. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 7s - loss: 0.2499 - accuracy: 0.8970 - val_loss: 0.4209 - val_accuracy: 0.8232\n",
      "Epoch 2/2\n",
      "196/196 - 7s - loss: 0.1630 - accuracy: 0.9377 - val_loss: 0.5078 - val_accuracy: 0.8158\n",
      "MLP Accuracy: 81.58%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data= (X_test, y_test), epochs =2,\n",
    "         batch_size =128, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MLP Accuracy: %.2f%%' %(scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\" size=6>4. CNN Unidimensional </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las mismas propiedades que hacen atractivo el modelo de CNN para aprender a reconocer objetos en imágenes pueden ayudar a aprender la estructura en párrafos de palabras, i.e., las técnicas de invarianza a la posición específica de las características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>4.1. Librería y dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras admite convoluciones unidimensionales y agrupación mediante las clases `Conv1D` y `MaxPooling1D`, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "vocab = 500\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words= vocab)\n",
    "\n",
    "max_words=500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4.2\"></a>\n",
    "# <font color=\"#004D7F\" size=5>4.2. Desarrollar la CNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos definir nuestro modelo de red neuronal convolucional. Esta vez:\n",
    "\n",
    "1. Después de la capa de entrada Embedding, insertamos una capa **Conv1D.** \n",
    "    * Esta capa convolucional tiene 32 mapas de características y lee representaciones de palabras embebidas 3 elementos vectoriales de la palabra embebida a la vez. \n",
    "2. Capa **MaxPooling1D** de 2x2. \n",
    "3. El resto de la red es igual que la red neuronal anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           16000     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,019,605\n",
      "Trainable params: 2,019,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model =Sequential()\n",
    "model.add(Embedding(vocab, 32, input_length = max_words))\n",
    "model.add(Conv1D(32,3, padding='same', activation= 'relu'))\n",
    "model.add(MaxPooling1D())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation= 'relu'))\n",
    "model.add(Dense(1, activation ='sigmoid'))\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4.3\"></a>\n",
    "# <font color=\"#004D7F\" size=5>4.3. Evaluación del modelo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 13s - loss: 0.5110 - accuracy: 0.7223 - val_loss: 0.3652 - val_accuracy: 0.8366\n",
      "Epoch 2/2\n",
      "196/196 - 12s - loss: 0.3476 - accuracy: 0.8479 - val_loss: 0.3491 - val_accuracy: 0.8453\n",
      "MLP Accuracy: 84.53%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data= (X_test, y_test), epochs =2,\n",
    "         batch_size =128, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('CNN Accuracy: %.2f%%' %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
