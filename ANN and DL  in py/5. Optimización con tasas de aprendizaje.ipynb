{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"#113D68\" size=6>Deep Learning con Python y Keras</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Parte 4. MLP avanzado</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=4>5. Optimización con tasas de aprendizaje</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. Introducción](#section1)\n",
    "* [2. Dataset Ionosphera](#section2)\n",
    "* [3. Tasa de aprendizaje basada en el tiempo](#section3)\n",
    "* [4. Tasa de aprendizaje basada en caídas (drop)](#section4)\n",
    "* [5. Consejos al utilizar la planificación de aprendizaje](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección, aprenderemos a usar una programación con diferentes de tasas de aprendizaje. Después de completar esta lección, sabrá:\n",
    "* El beneficio de programar tasas de aprendizaje.\n",
    "* Cómo configurar y evaluar una tasa de aprendizaje basada en el tiempo.\n",
    "* Cómo configurar y evaluar una tasa de aprendizaje basado en _drop._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Eliminar warning\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. Introducción</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La adaptación más simple y quizás más utilizada de las tasas de aprendizaje durante el entrenamiento son las técnicas que reducen la tasa de aprendizaje con el tiempo. Estos tienen la ventaja de realizar grandes cambios al comienzo del procedimiento de entrenamiento cuando se utilizan valores de tasa de aprendizaje más grandes, y disminuyen la tasa de aprendizaje de manera que se realicen una tasa menor y, por lo tanto, actualizaciones de entrenamiento más pequeñas a los pesos más adelante en el procedimiento de entrenamiento. \n",
    "\n",
    "Dos planificaciones de tasas de aprendizaje populares son:\n",
    "* Disminución de la tasa de aprendizaje gradualmente según la época.\n",
    "* Disminución de la tasa de aprendizaje usando grandes drops en épocas específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. Dataset Ionosphera</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un problema de clasificación binaria donde los casos positivos _(g_ para el bien) muestran evidencia de algún tipo de estructura en la ionosfera y los casos negativos _(b_ para el mal) no. \n",
    "\n",
    "Hay 34 atributos y 351 observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre el dataset [Ionosphere](https://archive.ics.uci.edu/ml/datasets/Ionosphere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. Tasa de aprendizaje basada en el tiempo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de optimización `SGD` tiene un argumento llamado `decay`. Este argumento se utiliza de la siguiente manera:\n",
    "\n",
    "$$\n",
    "    LearningRate = LearningRate \\times \\frac{1}{1+decay \\times epoch}\n",
    "$$\n",
    "\n",
    "Cuando el argumento de `decay` es cero (el valor predeterminado), esto no tiene ningún efecto en la tasa de aprendizaje (por ejemplo, 0,1).\n",
    "\n",
    "```\n",
    "    LearningRate = (0.1 * 1)/(1 + 0.0 * 1)\n",
    "    LearningRate = 0.1\n",
    "```\n",
    "\n",
    "Cuando se especifica el argumento `decay`, disminuirá la tasa de aprendizaje de la época anterior en la cantidad fija dada. Por ejemplo, si usamos el valor de la tasa de aprendizaje inicial de 0.1 y la decay de 0.001, las primeras 5 épocas adaptarán la tasa de aprendizaje de la siguiente manera:\n",
    "```\n",
    "    Epoch Learning Rate\n",
    "      1    0.1\n",
    "      2    0.0999000999\n",
    "      3    0.0997006985\n",
    "      4    0.09940249103\n",
    "      5    0.09900646517\n",
    "\n",
    "```\n",
    "\n",
    "Extender esto a 100 épocas producirá el siguiente gráfico de tasa de aprendizaje (eje y) versus época (eje x):\n",
    "\n",
    "<img src=\"images/learningRate.png\" width=\"350\" height=\"350\" />\n",
    "\n",
    "Puede crear una buena planificación predeterminada configurando el valor de `decay` de la siguiente manera:\n",
    "```\n",
    "    Decay = LearningRate / Epochs\n",
    "    Decay = 0.1 / 100\n",
    "    Decay = 0.001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo en un modelo con las siguientes características \n",
    "1. Red neuronal con 1 capa oculta con 34 neuronas y utilizando la función de activación ReLu. \n",
    "2. La capa de salida tiene una sola neurona y usa la función de activación sigmoidea. \n",
    "3. La tasa de aprendizaje para el `SGD` se ha establecido en un valor más alto de 0,1. \n",
    "4. Entrenamiento para 50 épocas y el argumento `decay` se ha establecido en 0,002, calculado como $\\frac{0.1}{50}$. \n",
    "6. Usamos un `momentum` de 0.8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 - 0s - loss: 0.6574 - accuracy: 0.6170 - val_loss: 0.5233 - val_accuracy: 0.8103\n",
      "Epoch 2/50\n",
      "9/9 - 0s - loss: 0.4621 - accuracy: 0.8511 - val_loss: 0.3556 - val_accuracy: 0.9052\n",
      "Epoch 3/50\n",
      "9/9 - 0s - loss: 0.3576 - accuracy: 0.8553 - val_loss: 0.4430 - val_accuracy: 0.8707\n",
      "Epoch 4/50\n",
      "9/9 - 0s - loss: 0.2765 - accuracy: 0.9149 - val_loss: 0.1915 - val_accuracy: 0.9483\n",
      "Epoch 5/50\n",
      "9/9 - 0s - loss: 0.2499 - accuracy: 0.9021 - val_loss: 0.2272 - val_accuracy: 0.9569\n",
      "Epoch 6/50\n",
      "9/9 - 0s - loss: 0.2011 - accuracy: 0.9447 - val_loss: 0.1597 - val_accuracy: 0.9741\n",
      "Epoch 7/50\n",
      "9/9 - 0s - loss: 0.1790 - accuracy: 0.9489 - val_loss: 0.1687 - val_accuracy: 0.9655\n",
      "Epoch 8/50\n",
      "9/9 - 0s - loss: 0.1628 - accuracy: 0.9447 - val_loss: 0.2027 - val_accuracy: 0.9569\n",
      "Epoch 9/50\n",
      "9/9 - 0s - loss: 0.1502 - accuracy: 0.9617 - val_loss: 0.1480 - val_accuracy: 0.9741\n",
      "Epoch 10/50\n",
      "9/9 - 0s - loss: 0.1455 - accuracy: 0.9532 - val_loss: 0.1933 - val_accuracy: 0.9569\n",
      "Epoch 11/50\n",
      "9/9 - 0s - loss: 0.1348 - accuracy: 0.9617 - val_loss: 0.0969 - val_accuracy: 0.9828\n",
      "Epoch 12/50\n",
      "9/9 - 0s - loss: 0.1271 - accuracy: 0.9702 - val_loss: 0.1561 - val_accuracy: 0.9741\n",
      "Epoch 13/50\n",
      "9/9 - 0s - loss: 0.1231 - accuracy: 0.9574 - val_loss: 0.0899 - val_accuracy: 0.9828\n",
      "Epoch 14/50\n",
      "9/9 - 0s - loss: 0.1281 - accuracy: 0.9617 - val_loss: 0.0822 - val_accuracy: 0.9828\n",
      "Epoch 15/50\n",
      "9/9 - 0s - loss: 0.1187 - accuracy: 0.9745 - val_loss: 0.0958 - val_accuracy: 0.9828\n",
      "Epoch 16/50\n",
      "9/9 - 0s - loss: 0.1131 - accuracy: 0.9660 - val_loss: 0.1439 - val_accuracy: 0.9828\n",
      "Epoch 17/50\n",
      "9/9 - 0s - loss: 0.1054 - accuracy: 0.9617 - val_loss: 0.0706 - val_accuracy: 0.9741\n",
      "Epoch 18/50\n",
      "9/9 - 0s - loss: 0.1012 - accuracy: 0.9745 - val_loss: 0.1306 - val_accuracy: 0.9828\n",
      "Epoch 19/50\n",
      "9/9 - 0s - loss: 0.0964 - accuracy: 0.9745 - val_loss: 0.1112 - val_accuracy: 0.9828\n",
      "Epoch 20/50\n",
      "9/9 - 0s - loss: 0.0917 - accuracy: 0.9702 - val_loss: 0.0748 - val_accuracy: 0.9828\n",
      "Epoch 21/50\n",
      "9/9 - 0s - loss: 0.0915 - accuracy: 0.9745 - val_loss: 0.0756 - val_accuracy: 0.9828\n",
      "Epoch 22/50\n",
      "9/9 - 0s - loss: 0.0894 - accuracy: 0.9745 - val_loss: 0.1510 - val_accuracy: 0.9483\n",
      "Epoch 23/50\n",
      "9/9 - 0s - loss: 0.0836 - accuracy: 0.9787 - val_loss: 0.0590 - val_accuracy: 0.9914\n",
      "Epoch 24/50\n",
      "9/9 - 0s - loss: 0.0848 - accuracy: 0.9702 - val_loss: 0.1185 - val_accuracy: 0.9828\n",
      "Epoch 25/50\n",
      "9/9 - 0s - loss: 0.0851 - accuracy: 0.9702 - val_loss: 0.0607 - val_accuracy: 0.9914\n",
      "Epoch 26/50\n",
      "9/9 - 0s - loss: 0.0857 - accuracy: 0.9745 - val_loss: 0.0799 - val_accuracy: 0.9914\n",
      "Epoch 27/50\n",
      "9/9 - 0s - loss: 0.0762 - accuracy: 0.9787 - val_loss: 0.0777 - val_accuracy: 0.9914\n",
      "Epoch 28/50\n",
      "9/9 - 0s - loss: 0.0747 - accuracy: 0.9830 - val_loss: 0.0825 - val_accuracy: 0.9914\n",
      "Epoch 29/50\n",
      "9/9 - 0s - loss: 0.0728 - accuracy: 0.9787 - val_loss: 0.0771 - val_accuracy: 0.9914\n",
      "Epoch 30/50\n",
      "9/9 - 0s - loss: 0.0732 - accuracy: 0.9787 - val_loss: 0.1105 - val_accuracy: 0.9914\n",
      "Epoch 31/50\n",
      "9/9 - 0s - loss: 0.0694 - accuracy: 0.9830 - val_loss: 0.0607 - val_accuracy: 0.9914\n",
      "Epoch 32/50\n",
      "9/9 - 0s - loss: 0.0670 - accuracy: 0.9830 - val_loss: 0.0872 - val_accuracy: 0.9914\n",
      "Epoch 33/50\n",
      "9/9 - 0s - loss: 0.0646 - accuracy: 0.9830 - val_loss: 0.0707 - val_accuracy: 0.9914\n",
      "Epoch 34/50\n",
      "9/9 - 0s - loss: 0.0636 - accuracy: 0.9830 - val_loss: 0.0697 - val_accuracy: 0.9914\n",
      "Epoch 35/50\n",
      "9/9 - 0s - loss: 0.0610 - accuracy: 0.9830 - val_loss: 0.0723 - val_accuracy: 0.9914\n",
      "Epoch 36/50\n",
      "9/9 - 0s - loss: 0.0603 - accuracy: 0.9787 - val_loss: 0.0716 - val_accuracy: 0.9914\n",
      "Epoch 37/50\n",
      "9/9 - 0s - loss: 0.0600 - accuracy: 0.9830 - val_loss: 0.0803 - val_accuracy: 0.9914\n",
      "Epoch 38/50\n",
      "9/9 - 0s - loss: 0.0593 - accuracy: 0.9830 - val_loss: 0.0775 - val_accuracy: 0.9914\n",
      "Epoch 39/50\n",
      "9/9 - 0s - loss: 0.0619 - accuracy: 0.9872 - val_loss: 0.0599 - val_accuracy: 0.9914\n",
      "Epoch 40/50\n",
      "9/9 - 0s - loss: 0.0585 - accuracy: 0.9872 - val_loss: 0.0818 - val_accuracy: 0.9914\n",
      "Epoch 41/50\n",
      "9/9 - 0s - loss: 0.0577 - accuracy: 0.9872 - val_loss: 0.0699 - val_accuracy: 0.9914\n",
      "Epoch 42/50\n",
      "9/9 - 0s - loss: 0.0551 - accuracy: 0.9872 - val_loss: 0.0663 - val_accuracy: 0.9914\n",
      "Epoch 43/50\n",
      "9/9 - 0s - loss: 0.0552 - accuracy: 0.9872 - val_loss: 0.0760 - val_accuracy: 0.9914\n",
      "Epoch 44/50\n",
      "9/9 - 0s - loss: 0.0542 - accuracy: 0.9830 - val_loss: 0.0571 - val_accuracy: 0.9914\n",
      "Epoch 45/50\n",
      "9/9 - 0s - loss: 0.0571 - accuracy: 0.9872 - val_loss: 0.0687 - val_accuracy: 0.9914\n",
      "Epoch 46/50\n",
      "9/9 - 0s - loss: 0.0527 - accuracy: 0.9872 - val_loss: 0.0612 - val_accuracy: 0.9914\n",
      "Epoch 47/50\n",
      "9/9 - 0s - loss: 0.0519 - accuracy: 0.9872 - val_loss: 0.0647 - val_accuracy: 0.9914\n",
      "Epoch 48/50\n",
      "9/9 - 0s - loss: 0.0542 - accuracy: 0.9872 - val_loss: 0.0708 - val_accuracy: 0.9914\n",
      "Epoch 49/50\n",
      "9/9 - 0s - loss: 0.0503 - accuracy: 0.9872 - val_loss: 0.0669 - val_accuracy: 0.9914\n",
      "Epoch 50/50\n",
      "9/9 - 0s - loss: 0.0501 - accuracy: 0.9872 - val_loss: 0.0669 - val_accuracy: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb17c3f3748>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Based Learning Rate Decay\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layes import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# load dataset\n",
    "r = \"C:/Users/Maria Luisa/OneDrive/Documentos/Cursos/DeepLearningConPythonyKerasRedesNeuronalesAvanzado/LearningRate/Datasets/ionosphere.csv\"\n",
    "df = pd.read_csv(r, header =None)\n",
    "df_values = df.values\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df_values[:,0:34].astype(float)\n",
    "y = df_values[:,34]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim = 34, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "epoch = 50\n",
    "lr = 0.1\n",
    "decay_rate = lr/epoch \n",
    "momentum = 0.8\n",
    "\n",
    "sgd = SGD(learning_rate = lr, momentum = momentum, decay = decay_rate ,\n",
    "         nesterov = False)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics= ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x,y, validation_split= 0.33, epochs = epoch, batch_size= 28,\n",
    "         verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\" size=6>4. Tasa de aprendizaje basada en caídas (drop)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A menudo, este método se implementa reduciendo la tasa de aprendizaje a la mitad cada número fijo de épocas. \n",
    "\n",
    "Por ejemplo, podemos tener una tasa de aprendizaje inicial de 0,1 y reducirla en un factor de 0,5 cada 10 épocas. Las primeras 10 épocas de entrenamiento usarían un valor de 0.1, en las siguientes 10 épocas se usaría una tasa de aprendizaje de 0.05, y así sucesivamente. \n",
    "\n",
    "El gráfico quedaría así con tasa de aprendizaje (eje y) frente a la época (eje x).\n",
    "\n",
    "<img src=\"images/learningRate2.png\" width=\"350\" height=\"350\" />\n",
    "\n",
    "Podemos implementar esto en Keras usando la llamada (callback) `LearningRateScheduler` al ajustar el modelo. `LearningRateScheduler` nos permite definir una función para llamar que toma el número de época como argumento y devuelve la tasa de aprendizaje para usar en el SGD.\n",
    "\n",
    "En el siguiente código, usamos el mismo ejemplo que antes de una sola red de capa oculta. Se define una nueva función `step_decay()` que implementa la ecuación:\n",
    "\n",
    "$$\n",
    "    LearningRate = InitialLearningRate \\times DropRate^{floor(\\frac{1+ Epoch }{EpochDrop})}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "* `InitialLearningRate` es la tasa de aprendizaje al comienzo de la ejecución \n",
    "* `EpochDrop` es la frecuencia con la que se reduce la tasa de aprendizaje en épocas y \n",
    "* `DropRate` es cuánto se reduce la tasa de aprendizaje cada vez que se elimina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 - 0s - loss: 0.6733 - accuracy: 0.6511 - val_loss: 0.4980 - val_accuracy: 0.7069\n",
      "Epoch 2/50\n",
      "9/9 - 0s - loss: 0.4207 - accuracy: 0.8383 - val_loss: 0.3068 - val_accuracy: 0.9569\n",
      "Epoch 3/50\n",
      "9/9 - 0s - loss: 0.2646 - accuracy: 0.9021 - val_loss: 0.2312 - val_accuracy: 0.9655\n",
      "Epoch 4/50\n",
      "9/9 - 0s - loss: 0.1984 - accuracy: 0.9319 - val_loss: 0.1994 - val_accuracy: 0.9483\n",
      "Epoch 5/50\n",
      "9/9 - 0s - loss: 0.2042 - accuracy: 0.9277 - val_loss: 0.1138 - val_accuracy: 0.9741\n",
      "Epoch 6/50\n",
      "9/9 - 0s - loss: 0.1553 - accuracy: 0.9489 - val_loss: 0.0795 - val_accuracy: 0.9655\n",
      "Epoch 7/50\n",
      "9/9 - 0s - loss: 0.1557 - accuracy: 0.9404 - val_loss: 0.2094 - val_accuracy: 0.9569\n",
      "Epoch 8/50\n",
      "9/9 - 0s - loss: 0.1217 - accuracy: 0.9574 - val_loss: 0.0877 - val_accuracy: 0.9741\n",
      "Epoch 9/50\n",
      "9/9 - 0s - loss: 0.1116 - accuracy: 0.9660 - val_loss: 0.0829 - val_accuracy: 0.9741\n",
      "Epoch 10/50\n",
      "9/9 - 0s - loss: 0.0946 - accuracy: 0.9660 - val_loss: 0.0984 - val_accuracy: 0.9828\n",
      "Epoch 11/50\n",
      "9/9 - 0s - loss: 0.0848 - accuracy: 0.9787 - val_loss: 0.0974 - val_accuracy: 0.9828\n",
      "Epoch 12/50\n",
      "9/9 - 0s - loss: 0.0822 - accuracy: 0.9787 - val_loss: 0.0801 - val_accuracy: 0.9741\n",
      "Epoch 13/50\n",
      "9/9 - 0s - loss: 0.0741 - accuracy: 0.9830 - val_loss: 0.0991 - val_accuracy: 0.9828\n",
      "Epoch 14/50\n",
      "9/9 - 0s - loss: 0.0703 - accuracy: 0.9830 - val_loss: 0.0751 - val_accuracy: 0.9741\n",
      "Epoch 15/50\n",
      "9/9 - 0s - loss: 0.0675 - accuracy: 0.9830 - val_loss: 0.0879 - val_accuracy: 0.9828\n",
      "Epoch 16/50\n",
      "9/9 - 0s - loss: 0.0660 - accuracy: 0.9787 - val_loss: 0.0767 - val_accuracy: 0.9741\n",
      "Epoch 17/50\n",
      "9/9 - 0s - loss: 0.0628 - accuracy: 0.9830 - val_loss: 0.0802 - val_accuracy: 0.9828\n",
      "Epoch 18/50\n",
      "9/9 - 0s - loss: 0.0606 - accuracy: 0.9830 - val_loss: 0.0775 - val_accuracy: 0.9741\n",
      "Epoch 19/50\n",
      "9/9 - 0s - loss: 0.0589 - accuracy: 0.9787 - val_loss: 0.0767 - val_accuracy: 0.9741\n",
      "Epoch 20/50\n",
      "9/9 - 0s - loss: 0.0578 - accuracy: 0.9830 - val_loss: 0.0665 - val_accuracy: 0.9741\n",
      "Epoch 21/50\n",
      "9/9 - 0s - loss: 0.0577 - accuracy: 0.9787 - val_loss: 0.0914 - val_accuracy: 0.9828\n",
      "Epoch 22/50\n",
      "9/9 - 0s - loss: 0.0548 - accuracy: 0.9830 - val_loss: 0.0754 - val_accuracy: 0.9828\n",
      "Epoch 23/50\n",
      "9/9 - 0s - loss: 0.0528 - accuracy: 0.9830 - val_loss: 0.0688 - val_accuracy: 0.9741\n",
      "Epoch 24/50\n",
      "9/9 - 0s - loss: 0.0534 - accuracy: 0.9830 - val_loss: 0.0685 - val_accuracy: 0.9741\n",
      "Epoch 25/50\n",
      "9/9 - 0s - loss: 0.0499 - accuracy: 0.9830 - val_loss: 0.0786 - val_accuracy: 0.9828\n",
      "Epoch 26/50\n",
      "9/9 - 0s - loss: 0.0512 - accuracy: 0.9830 - val_loss: 0.0769 - val_accuracy: 0.9828\n",
      "Epoch 27/50\n",
      "9/9 - 0s - loss: 0.0490 - accuracy: 0.9830 - val_loss: 0.0681 - val_accuracy: 0.9828\n",
      "Epoch 28/50\n",
      "9/9 - 0s - loss: 0.0494 - accuracy: 0.9830 - val_loss: 0.0672 - val_accuracy: 0.9828\n",
      "Epoch 29/50\n",
      "9/9 - 0s - loss: 0.0493 - accuracy: 0.9830 - val_loss: 0.0757 - val_accuracy: 0.9828\n",
      "Epoch 30/50\n",
      "9/9 - 0s - loss: 0.0475 - accuracy: 0.9872 - val_loss: 0.0733 - val_accuracy: 0.9828\n",
      "Epoch 31/50\n",
      "9/9 - 0s - loss: 0.0464 - accuracy: 0.9872 - val_loss: 0.0684 - val_accuracy: 0.9828\n",
      "Epoch 32/50\n",
      "9/9 - 0s - loss: 0.0464 - accuracy: 0.9830 - val_loss: 0.0673 - val_accuracy: 0.9828\n",
      "Epoch 33/50\n",
      "9/9 - 0s - loss: 0.0462 - accuracy: 0.9872 - val_loss: 0.0703 - val_accuracy: 0.9828\n",
      "Epoch 34/50\n",
      "9/9 - 0s - loss: 0.0455 - accuracy: 0.9830 - val_loss: 0.0680 - val_accuracy: 0.9828\n",
      "Epoch 35/50\n",
      "9/9 - 0s - loss: 0.0451 - accuracy: 0.9830 - val_loss: 0.0688 - val_accuracy: 0.9828\n",
      "Epoch 36/50\n",
      "9/9 - 0s - loss: 0.0451 - accuracy: 0.9830 - val_loss: 0.0703 - val_accuracy: 0.9828\n",
      "Epoch 37/50\n",
      "9/9 - 0s - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0700 - val_accuracy: 0.9828\n",
      "Epoch 38/50\n",
      "9/9 - 0s - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0696 - val_accuracy: 0.9828\n",
      "Epoch 39/50\n",
      "9/9 - 0s - loss: 0.0443 - accuracy: 0.9872 - val_loss: 0.0701 - val_accuracy: 0.9828\n",
      "Epoch 40/50\n",
      "9/9 - 0s - loss: 0.0432 - accuracy: 0.9872 - val_loss: 0.0669 - val_accuracy: 0.9828\n",
      "Epoch 41/50\n",
      "9/9 - 0s - loss: 0.0435 - accuracy: 0.9872 - val_loss: 0.0657 - val_accuracy: 0.9828\n",
      "Epoch 42/50\n",
      "9/9 - 0s - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.0659 - val_accuracy: 0.9828\n",
      "Epoch 43/50\n",
      "9/9 - 0s - loss: 0.0430 - accuracy: 0.9872 - val_loss: 0.0673 - val_accuracy: 0.9828\n",
      "Epoch 44/50\n",
      "9/9 - 0s - loss: 0.0440 - accuracy: 0.9872 - val_loss: 0.0738 - val_accuracy: 0.9828\n",
      "Epoch 45/50\n",
      "9/9 - 0s - loss: 0.0431 - accuracy: 0.9872 - val_loss: 0.0719 - val_accuracy: 0.9828\n",
      "Epoch 46/50\n",
      "9/9 - 0s - loss: 0.0425 - accuracy: 0.9872 - val_loss: 0.0690 - val_accuracy: 0.9828\n",
      "Epoch 47/50\n",
      "9/9 - 0s - loss: 0.0424 - accuracy: 0.9872 - val_loss: 0.0678 - val_accuracy: 0.9828\n",
      "Epoch 48/50\n",
      "9/9 - 0s - loss: 0.0423 - accuracy: 0.9872 - val_loss: 0.0662 - val_accuracy: 0.9828\n",
      "Epoch 49/50\n",
      "9/9 - 0s - loss: 0.0427 - accuracy: 0.9872 - val_loss: 0.0651 - val_accuracy: 0.9828\n",
      "Epoch 50/50\n",
      "9/9 - 0s - loss: 0.0425 - accuracy: 0.9872 - val_loss: 0.0674 - val_accuracy: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb10c1b6f28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Based Learning Rate Decay\n",
    "import pandas as pd\n",
    "import math as mth\n",
    "from keras.models import Sequential\n",
    "from keras.layes import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "#planificar la tasa de aprendizaje\n",
    "def step_decay(epoch):\n",
    "    initial_rate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop=10\n",
    "    lrate = initial_rate * mth.pow(drop, mth.floor(1 + epoch) / epochs_drop)\n",
    "    return lrate\n",
    "\n",
    "# load dataset\n",
    "r = \"C:/Users/Maria Luisa/OneDrive/Documentos/Cursos/DeepLearningConPythonyKerasRedesNeuronalesAvanzado/LearningRate/Datasets/ionosphere.csv\"\n",
    "df = pd.read_csv(r, header =None)\n",
    "df_values = df.values\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df_values[:,0:34].astype(float)\n",
    "y = df_values[:,34]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim = 34, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "\n",
    "sgd = SGD(learning_rate =0.0, momentum = 0.9)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics= ['accuracy'])\n",
    "\n",
    "#planificador de aprendizaje\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callback_list = [lrate]\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x,y, validation_split= 0.33, epochs = epoch, batch_size= 28,\n",
    "         callbacks = callback_list, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "# <font color=\"#004D7F\" size=6>5. Consejos al utilizar la planificación de aprendizaje</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos algunos consejos a tener en cuenta al utilizar la planficación de la tasa de aprendizaje.\n",
    "* **Incrementar la tasa de aprendizaje inicial**. Debido a que la tasa de aprendizaje disminuirá, comience con un valor mayor desde el cual disminuir.\n",
    "* **Utilice un gran momentum**.\n",
    "* **Experimente con diferentes planificaciones**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
